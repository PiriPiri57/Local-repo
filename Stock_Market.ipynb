{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiriPiri57/Local-repo/blob/main/Stock_Market.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh5GFuWwVz09"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install yfinance\n",
        "!pip install yfinance --quiet\n",
        "!pip install pandas_market_calendars --quiet\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas_market_calendars as mcal\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Choose your stock and time period\n",
        "ticker = 'RELIANCE.NS'  # You can change this to any valid stock symbol\n",
        "stock = yf.Ticker(ticker)\n",
        "nse = mcal.get_calendar('NSE')\n",
        "\n",
        "# Step 4: Download historical data\n",
        "today = datetime.datetime.today()\n",
        "\n",
        "# If today is Monday (0), we need Friday's date (today - 3 days)\n",
        "# If today is Sunday (6), we need Friday's date (today - 2 days)\n",
        "# If today is Saturday (5), we need Friday's date (today - 1 day)\n",
        "# Otherwise (Tuesday to Friday), just take yesterday\n",
        "\n",
        "start = today - datetime.timedelta(days=10)\n",
        "end = today\n",
        "\n",
        "schedule = nse.schedule(start_date=start.strftime('%Y-%m-%d'), end_date=end.strftime('%Y-%m-%d'))\n",
        "trading_days = schedule.index\n",
        "\n",
        "# Find the last trading day before or equal to today\n",
        "if today.date() in trading_days.date:\n",
        "    last_working_day = today.date()\n",
        "else:\n",
        "    last_working_day = trading_days[trading_days < np.datetime64(today)].max().date()\n",
        "\n",
        "print(f\"Last working day: {last_working_day}\")\n",
        "\n",
        "# Step 2: Download data up to the LAST working day (INCLUDE it)\n",
        "df = yf.download(ticker, end=(last_working_day + datetime.timedelta(days=1)).strftime('%Y-%m-%d'))\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# Save entire dataset\n",
        "save_path = \"/content/drive/My Drive/DS_Dataset_FinanceTrends.csv\"\n",
        "df.to_csv(save_path, index=True)\n",
        "\n",
        "# Step 3: Save only last working day's row into another dataset (today_features)\n",
        "df_copy=df.copy()\n",
        "\n",
        "\n",
        "# Step 5: Display first few rows\n",
        "print(\"First rows of stock data:\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Step 6: Plot the Close price\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Close'], label='Close Price', color='blue')\n",
        "plt.title(f\"{ticker} Stock Price Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "bW5RMxTd_xTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "OHjIpJt1DGAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "u799sjQECp37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "scaler = StandardScaler()\n",
        "df['Volume_scaled'] = scaler.fit_transform(df[['Volume']])\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(df['Date'], df['Volume_scaled'], color='blue')\n",
        "plt.title('Volume over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volume')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XHwg6VtIFOji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Dividends', 'Stock Splits'], inplace=True, errors='ignore') #Removing constant columns\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "L_bdFogzGlTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# --- 1. Date Parts ---\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "\n",
        "# --- 2. Lag Features ---\n",
        "df['lag_1'] = df['Close'].shift(1)  # yesterday's close\n",
        "df['lag_5'] = df['Close'].shift(5)  # close 5 days ago\n",
        "df['lag_10'] = df['Close'].shift(10)\n",
        "\n",
        "# --- 3. Rolling Means ---\n",
        "df['rolling_mean_5'] = df['Close'].rolling(window=5).mean()\n",
        "df['rolling_mean_10'] = df['Close'].rolling(window=10).mean()\n",
        "df['rolling_mean_20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "\n",
        "# --- 4. Returns ---\n",
        "df['Daily_Return'] = df['Close'].pct_change()\n",
        "df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "\n",
        "df['Log_Volume'] = df['Volume'].shift(1)\n",
        "df['Rolling_Volume'] = df['Volume'].rolling(3).mean()\n",
        "\n",
        "\n",
        "# --- Drop NaNs caused by shift/rolling ---\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "njyrXrxpIQ9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw=df.copy()\n",
        "df.drop(columns=['Date'], inplace=True, errors='ignore')  #removing categorical columns, doesnt has any significant affect on data no point of encoding it\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HMUM6O0dUKDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(8)"
      ],
      "metadata": {
        "id": "rXh_c-mJJMeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "sns.heatmap(data=df.corr(),annot=True)"
      ],
      "metadata": {
        "id": "Jl5249xDLUB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['lag_5','lag_10','Open','Low','Rolling_Volume','Daily_Return','IsMonthStart','rolling_mean_10','rolling_mean_20'], inplace=True, errors='ignore')   #we are dropping the columns which are higly correlated to avoid overfitting\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_VGKrNTKN1pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "sns.heatmap(data=df.corr(),annot=True)"
      ],
      "metadata": {
        "id": "eSj1DjYNSrG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import seaborn as sns\n",
        "scaler = StandardScaler()\n",
        "df[['High', 'Close', 'Volume', 'lag_1', 'rolling_mean_5', 'Log_Return']] = scaler.fit_transform(df[['High', 'Close', 'Volume', 'lag_1', 'rolling_mean_5', 'Log_Return']])\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "HxvOHOw_TgsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Close\"].describe()"
      ],
      "metadata": {
        "id": "DXBXy99hcPoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(df['Close'])                              #Checking for outliers cause for stock market outliers indicate trend breakers\n",
        "sns.displot(df['Close'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YNIUrtwu9_0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "today_data=df_raw.iloc[-1]\n",
        "df=df.iloc[:-1]\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "cln14GJsrS-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "today_data_df = pd.DataFrame([today_data])\n",
        "today_data_df.drop(columns=['Date','lag_5','lag_10','Open','Low','Rolling_Volume','Daily_Return','IsMonthStart','rolling_mean_10','rolling_mean_20'], inplace=True, errors='ignore')\n",
        "today_data_df.describe()"
      ],
      "metadata": {
        "id": "Vk19X9KnuyYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# --- 1. Prepare Data ---\n",
        "\n",
        "# Define X and y\n",
        "X = df.drop(['Close'], axis=1)  # Drop target\n",
        "y = df['Close']\n",
        "\n",
        "# Sort the data by date (Ensure your data is sorted by date/time)\n",
        "df = df.sort_index()  # Assuming your dataframe is indexed by date\n",
        "\n",
        "# Split into train and test (chronologically)\n",
        "train_size = int(len(df) * 0.8)  # 80% training data\n",
        "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# --- 2. Define Models ---\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Lasso Regression': Lasso(alpha=0.1),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
        "}\n",
        "\n",
        "# --- 3. Train and Evaluate ---\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'R² Score': r2\n",
        "    }\n",
        "\n",
        "# --- 4. Display Results ---\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "jyJgFHkzWw4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = results_df['R² Score'].idxmax()\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"Best Model Selected: {best_model_name}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use a default style\n",
        "plt.style.use('ggplot')  # or 'default', 'bmh', etc.\n",
        "\n",
        "# Metrics you want to plot\n",
        "metrics = ['MAE', 'RMSE', 'R² Score']\n",
        "\n",
        "# Create a figure with 3 subplots (1 row, 3 columns)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))  # Width=20, Height=6\n",
        "\n",
        "# Loop through each metric and corresponding axis\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx]\n",
        "    sorted_data = results_df[metric].sort_values(ascending=True if metric != 'R² Score' else False)\n",
        "    sorted_data.plot(kind='barh', ax=ax, color='skyblue')\n",
        "    ax.set_title(f'Model Comparison - {metric}')\n",
        "    ax.set_xlabel(metric)\n",
        "    ax.set_ylabel('Models')\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NSngQTAXaIBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From the above plot we infer that Ridge and Linear Regression are the best fit"
      ],
      "metadata": {
        "id": "o5LWBVt4bKqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare today's data properly ---\n",
        "\n",
        "# Step 1: Convert today_data to a DataFrame\n",
        "\n",
        "##########################################\n",
        "# Step 5: Fill any NaNs\n",
        "today_data_df.fillna(0, inplace=True)\n",
        "X=df.drop(['Close'], axis=1)\n",
        "feature_names = X.columns\n",
        "\n",
        "# Step 2: Keep only the columns you trained on\n",
        "today_data_df = today_data_df[feature_names]\n",
        "\n",
        "# Step 3: Now safely scale\n",
        "today_data_scaled = scaler.transform(today_data_df)\n",
        "\n",
        "# Step 7: Predict today's close\n",
        "today_prediction = best_model.predict(today_data_df)\n",
        "predicted_close = today_prediction.flatten()[0]\n",
        "\n",
        "# Step 8: Get actual close\n",
        "actual_close = today_data['Close'].item()\n",
        "\n",
        "# Step 9: Calculate accuracy\n",
        "accuracy = (1 - abs(actual_close - predicted_close) / actual_close) * 100\n",
        "\n",
        "# Step 10: Display prediction\n",
        "print(\"Today's Close Prediction Results\")\n",
        "print(f\"Predicted Close Value ({best_model_name}): {predicted_close:.2f}\")\n",
        "print(f\"Actual Close Value: {actual_close:.2f}\")\n",
        "print(f\"Prediction Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "MSfh_P9us69_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Xh8rwoEdiy8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gain/Fall'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n",
        "df.tail()\n",
        "\n",
        "plt.pie(df['Gain/Fall'].value_counts().values,\n",
        "        labels=[\"Fall\", \"Gain\"], autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "# Step 1: Features and Target for classification\n",
        "X_cls = df.drop(['Close', 'Gain/Fall'], axis=1)  # Use all features except Close and Gain/Fall make it greeej\n",
        "y_cls = df['Gain/Fall']                          # Target is Gain/Fall\n",
        "\n",
        "# Train-Test Split\n",
        "train_size_cls = int(len(df) * 0.8)\n",
        "X_train_cls, X_test_cls = X_cls.iloc[:train_size_cls], X_cls.iloc[train_size_cls:]\n",
        "y_train_cls, y_test_cls = y_cls.iloc[:train_size_cls], y_cls.iloc[train_size_cls:]"
      ],
      "metadata": {
        "id": "SN5IIyC5226E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "models = [LogisticRegression(),\n",
        "  XGBClassifier(),\n",
        "  RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "  DecisionTreeClassifier(random_state=42),\n",
        "  KNeighborsClassifier(n_neighbors=5),\n",
        "  GradientBoostingClassifier(n_estimators=100, random_state=42)]\n",
        "best_score = 0\n",
        "best_classifier = None\n",
        "for i in range(6):\n",
        "  models[i].fit(X_train_cls, y_train_cls)\n",
        "  train_auc = metrics.roc_auc_score(\n",
        "  y_train_cls, model.predict_proba(X_train_cls)[:, 1])\n",
        "  val_auc = metrics.roc_auc_score(\n",
        "  y_test_cls, model.predict_proba(X_test_cls)[:, 1])\n",
        "  print(f'{models[i].__class__.__name__} : ')\n",
        "  print('Training Accuracy : ', train_auc)\n",
        "  print('Validation Accuracy : ',val_auc)\n",
        "  print()\n",
        "  if val_auc > best_score:\n",
        "      best_score = val_auc\n",
        "      best_classifier = model\n",
        "\n",
        "best_classifier.__class__.__name__"
      ],
      "metadata": {
        "id": "AgmHy4HkXd1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a grid of subplots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 10))  # 2 rows and 4 columns (since you have 7 models, some axes will be empty)\n",
        "axes = axes.ravel()  # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Loop through all models and plot the confusion matrix for each\n",
        "for i, model in enumerate(models):\n",
        "    # Get the predicted classes\n",
        "    y_pred = model.predict(X_test_cls)\n",
        "\n",
        "    # Plot the confusion matrix on the corresponding subplot\n",
        "    ConfusionMatrixDisplay.from_estimator(\n",
        "        model, X_test_cls, y_test_cls, ax=axes[i], cmap='Blues', display_labels=model.classes_)\n",
        "\n",
        "    axes[i].set_title(f'{model.__class__.__name__}')\n",
        "    axes[i].set_xlabel('Predicted')\n",
        "    axes[i].set_ylabel('True')\n",
        "    axes[i].grid(False)\n",
        "\n",
        "# Adjust layout for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wWiZsFKZcFa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure for a single plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Loop through all models and plot the ROC curve for each\n",
        "for model in models:\n",
        "    # Get predicted probabilities (not just 0/1 predictions)\n",
        "    y_probs = model.predict_proba(X_test_cls)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, thresholds = roc_curve(y_test_cls, y_probs)\n",
        "    auc_score = roc_auc_score(y_test_cls, y_probs)\n",
        "\n",
        "    # Plot ROC curve on the same plot\n",
        "    plt.plot(fpr, tpr, label=f'{model.__class__.__name__} (AUC = {auc_score:.3f})')\n",
        "\n",
        "# Plot random chance diagonal (AUC = 0.5)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Chance (AUC = 0.5)\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('ROC Curves for All Models')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# Add legend\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Add grid\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ShVokbsmZ6ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "today_features = today_data_df.values\n",
        "\n",
        "# Predict probability of gain (class 1)\n",
        "prob_gain = best_classifier.predict_proba(today_features)[0][1]  # Probability of Gain\n",
        "\n",
        "# Make final prediction (with threshold)\n",
        "prediction = 1 if prob_gain > 0.4 else 0  # You can adjust threshold\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "d0c48_d65LjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PURE ARIMA MODEL (without pmdarima) ---\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Install if needed\n",
        "!pip install statsmodels --quiet\n",
        "\n",
        "# Import libraries\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Prepare the series\n",
        "series = df_raw['Close']\n",
        "!pip install statsmodels --quiet\n",
        "\n",
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Plot ACF and PACF\n",
        "plt.figure(figsize=(16,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plot_acf(series, lags=50, ax=plt.gca())\n",
        "plt.title('Autocorrelation Function (ACF)')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plot_pacf(series, lags=100, ax=plt.gca(), method='ywm')\n",
        "plt.title('Partial Autocorrelation Function (PACF)')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Split into train/test\n",
        "train_size = int(len(series) * 0.8)\n",
        "train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "# --- Choose ARIMA order manually ---\n",
        "# Common starting point: (p,d,q) = (5,1,0) or (2,1,2)\n",
        "# You can tune these based on ACF/PACF plots if you want!\n"
      ],
      "metadata": {
        "id": "76eS-QUg1tK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install if needed\n",
        "!pip install statsmodels --quiet\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Prepare the series\n",
        "series = df_raw['Close']\n",
        "\n",
        "# Split into train/test\n",
        "train_size = int(len(series) * 0.8)\n",
        "train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "# --- Set SARIMA orders ---\n",
        "order = (1, 0, 1)\n",
        "seasonal_order = (1, 0, 1, 24)      #seasonality is 24 that is 2 years\n",
        "\n",
        "# --- Train SARIMA Model ---\n",
        "sarima_model = SARIMAX(train,\n",
        "                       order=order,\n",
        "                       seasonal_order=seasonal_order,\n",
        "                       enforce_stationarity=False,\n",
        "                       enforce_invertibility=False)\n",
        "sarima_result = sarima_model.fit()\n",
        "\n",
        "# --- Forecast ---\n",
        "n_periods = len(test)\n",
        "forecast = sarima_result.forecast(steps=n_periods)\n",
        "\n",
        "# --- Evaluate ---\n",
        "rmse = np.sqrt(mean_squared_error(test, forecast))\n",
        "print(f'SARIMA RMSE: {rmse:.2f}')\n",
        "\n",
        "# --- Plot ---\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(train.index, train, label='Train')\n",
        "plt.plot(test.index, test, color='green', label='Test')\n",
        "plt.plot(test.index, forecast, color='red', label='Forecast (SARIMA)')\n",
        "plt.title('SARIMA Forecast vs Actuals')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MB8viRzg5tV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Predict Today's Close Price using SARIMA ---\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Fit on FULL dataset\n",
        "full_sarima_model = SARIMAX(series,\n",
        "                            order=order,\n",
        "                            seasonal_order=seasonal_order,\n",
        "                            enforce_stationarity=False,\n",
        "                            enforce_invertibility=False)\n",
        "full_sarima_result = full_sarima_model.fit()\n",
        "\n",
        "# Forecast next step\n",
        "today_forecast = sarima_result.forecast(steps=1)\n",
        "predicted_today_close = today_forecast.values[0]\n",
        "\n",
        "# Actual today close (from today's stored data)\n",
        "actual_today_close = today_data['Close'].item()\n",
        "\n",
        "# Accuracy\n",
        "sarima_accuracy = (1 - abs(actual_today_close - predicted_today_close) / actual_today_close) * 100\n",
        "\n",
        "print(\"Today's Close Prediction Results with SARIMA\")\n",
        "print(f\"Predicted Close Value (SARIMA): {predicted_today_close:.2f}\")\n",
        "print(f\"Actual Close Value: {actual_today_close:.2f}\")\n",
        "print(f\"Prediction Accuracy: {sarima_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "jbkqhZ_n549g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Sanitize y_test and y_pred\n",
        "def generate_signals(y_true, y_pred, threshold=0.01):\n",
        "    signals = []\n",
        "    for actual, pred in zip(y_true, y_pred):\n",
        "        change = (pred - actual) / actual\n",
        "        if change > threshold:\n",
        "            signals.append('Buy')\n",
        "        elif change < -threshold:\n",
        "            signals.append('Sell')\n",
        "        else:\n",
        "            signals.append('Hold')\n",
        "    return signals\n",
        "\n",
        "# Step 2: Make Predictions and Flatten if Needed\n",
        "\n",
        "\n",
        "y_true = y_test.values.flatten()\n",
        "y_pred = best_model.predict(X_test)\n",
        "if len(y_pred.shape) > 1:\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "# Match lengths if needed\n",
        "min_length = min(len(y_true), len(y_pred))\n",
        "y_true = y_true[:min_length]\n",
        "y_pred = y_pred[:min_length]\n",
        "\n",
        "# Step 1: Generate signals\n",
        "signals = generate_signals(y_true, y_pred)\n",
        "\n",
        "# Step 2: Build the DataFrame\n",
        "df_signals = pd.DataFrame({\n",
        "    'Actual_Close': y_true,\n",
        "    'Predicted_Close': y_pred,\n",
        "    'Signal': signals\n",
        "})\n",
        "df_signals.index = y_test.index[:min_length]  # Safe indexing\n",
        "def backtest_strategy(df, initial_cash=1000):\n",
        "    cash = initial_cash\n",
        "    shares = 0\n",
        "    portfolio_values = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        signal = row['Signal']\n",
        "        price = row['Actual_Close']\n",
        "\n",
        "        if signal == 'Buy' and cash > 0:\n",
        "            shares = cash / price\n",
        "            cash = 0\n",
        "\n",
        "        elif signal == 'Sell' and shares > 0:\n",
        "            cash = shares * price\n",
        "            shares = 0\n",
        "\n",
        "        portfolio_value = cash + shares * price\n",
        "        portfolio_values.append(portfolio_value)\n",
        "\n",
        "    return portfolio_values\n",
        "\n",
        "# Step 5: Run Backtest\n",
        "portfolio_values = backtest_strategy(df_signals)\n",
        "df_signals['Portfolio_Value'] = portfolio_values\n",
        "\n",
        "# Step 6: Show Final Results\n",
        "final_value = portfolio_values[-1]\n",
        "total_return = (final_value - 1000) / 1000 * 100\n",
        "print(f\"\\nFinal Portfolio Value: ₹{final_value:.2f}\")\n",
        "print(f\" Total Strategy Return: {total_return:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ee5DHGqOwBZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(df_signals.index, df_signals['Portfolio_Value'], label='Portfolio Value', color='blue')\n",
        "plt.title('Portfolio Growth Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Portfolio Value (₹)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 8: Plot Signal Distribution\n",
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(x='Signal', data=df_signals)\n",
        "plt.title('Distribution of Buy/Sell/Hold Signals')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "somSmvEcwuc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Make sure you have the original Date\n",
        "# Assuming 'df_raw' is available and it has the 'Date' column\n",
        "# If not, you must reload your original data with dates!\n",
        "\n",
        "# Step 2: Get last known Date\n",
        "last_date = pd.to_datetime(df_copy['Date'].iloc[-1])  # Last available real date\n",
        "\n",
        "# Step 3: Forecast next 30 business days (using SARIMA)\n",
        "# 'full_sarima_result' must already be trained\n",
        "future_forecast = full_sarima_result.forecast(steps=365)\n",
        "\n",
        "# Step 4: Create 30 future business days\n",
        "future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=365, freq='B')\n",
        "\n",
        "# Step 5: Organize Forecast into DataFrame\n",
        "future_forecast_df = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'Predicted_Close': future_forecast.values\n",
        "})\n",
        "future_forecast_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Step 6: Plot Historical and Future Prices Together\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "# Plot historical close prices\n",
        "plt.plot(df_copy['Date'], df_copy['Close'], label='Historical Close Prices')\n",
        "\n",
        "# Plot future forecasted prices\n",
        "plt.plot(future_forecast_df.index, future_forecast_df['Predicted_Close'], label='365-Day Forecast', color='green')\n",
        "\n",
        "plt.title('Future 365 Days Stock Price Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Show the Forecast Table\n",
        "print(\"\\n365-Day Future Stock Price Forecast:\")\n",
        "print(future_forecast_df.head())\n",
        "print(\"\\n\",future_forecast_df.tail())"
      ],
      "metadata": {
        "id": "bfViSEw11JIl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}